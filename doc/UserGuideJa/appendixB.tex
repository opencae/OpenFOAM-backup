%#! platex UserGuideJa
\chapter{その他の参考情報}
\label{chap:B}
現在本章は，ユーザガイドの主要部分に入れるのに相応わしくないと
思われる情報の置き場になっており，
それは例えば，情報が細かすぎるとか，古いとかの理由なのですが，
特定の状況ではユーザの役に立つかもしれません．



\section{MPICHを用いた領域分解ケースの並列実行}
\label{sec:B.1}
\index{MPICH@\OFemph{MPICH}!メッセージパッシングインターフェイス}%
\index{メッセージパッシングインターフェイス!MPICH@\OFemph{MPICH}}%
\index{MPICH@\OFemph{MPICH}!MPI}%
\index{MPI!MPICH@\OFemph{MPICH}}%
この節では，\autoref{ssec:3.4.2}で説明した\OFemph{openMPI}ではなく，
MPI/\OFemph{MPICH}を使ってOpenFOAMのケースを並列計算する方法を説明します．

実行するアプリケーションがすべてのプロセッサノードに
同じパス名をもっているかどうかで，MPI/\OFemph{MPICH}の起動は異なります．
以下の二つの場合では，実行ファイルへのパス名は異なります．
\begin{itemize}
 \item プロセッサが全て同じUNIX/Linux構造に属さない場合
 \item すべてのノードから実行ファイルへのアクセスができる
       ネットワークファイルシステム (NFS) がなく，
       したがって，それは異なったノードの
       異なった場所にインストールされる場合
\end{itemize}


\subsection{全てのノードで同じ実行パス名の場合}
\label{ssec:B.1.1}
プロセッサノードがすべてローカルである単一マシンにおいて，
以下のコマンドを実行します．
その際は \verb|`| は逆向きの引用文字であり
一般的にはキーボードの最上部右側にある（\verb|'| ではない）
ことに気をつけてください．
\begin{OFterminal}
\begin{verbatim}
mpirun -np <nProcs> `which <foamExec>`
  <otherArgs> -parallel < /dev/null >& log &
\end{verbatim}
\end{OFterminal}
\verb|<nProcs>| はプロセッサの数です．\verb|<foamExec>| は実行ファイル，
例えば，\OFtool{icoFoam}です．
そして，出力は\OFpath{log}というファイルに転記されます．
例えば，\OFtool{icoFoam}が，
\OFpath{\$FOAM\_RUN/tutorials/icoFoam}ディレクトリの
キャビティチュートリアルを三つのノードで実行する場合は，
以下のコマンドを実行します．
\begin{OFterminal}
\begin{verbatim}
mpirun -np 3 `which icoFoam` $FOAM_RUN/tutorials/icoFoam cavity
  -parallel < /dev/null >& log &
\end{verbatim}%$
\end{OFterminal}
アクセスしたいプロセッサがマシンのクラスタの向こう側にあるとき，
以下のコマンドを実行します．
\begin{OFterminal}
\begin{verbatim}
mpirun  -machinefile <machinesFile> -np <nProcs> `which <foamExec>`
<otherArgs> -parallel < /dev/null >& log &
\end{verbatim}
\end{OFterminal}
ノード名を含む \verb|<machinesFile>| ファイルが各行にひとつあり，
現在ユーザがログオンしているマシンが最初のものであることを除いて，
従来と同じです．\verb|<machinesFile>| は，MPICHで読まれるファイルであり，
したがって，マシン名と各マシンで使用するプロセッサ数が必要なだけで，
ヘッダは必要ありません．例えば，マシンarpで1プロセッサ，
マシンnoddyで2プロセッサ実行するには，ファイルは以下のようにします．
\begin{OFfile}
\begin{verbatim}
arp:1
noddy:2
\end{verbatim}
\end{OFfile}
注意：共有メモリをもつマシンのクラスタにおける性能の最適化は，
MPICHライブラリを再コンパイルする必要があるかもしれません．
やり方はMPICH資料を見てください．


\subsection{ノード間で実行パス名が異なる場合}
\label{ssec:B.1.2}
異なるパス名をもつ実行ファイルを異なるノードで実行するには，
全てのノードに同じバーションのOpenFOAMをインストールし
rshを使用して実行する能力が必要です．後者は全てのノードで，
例えば\OFtool{icoFoam}のようなアプリケーションを実行することで確認できます．
\begin{OFterminal}
\begin{verbatim}
rsh <machineName> icoFoam <root> <case>
\end{verbatim}
\end{OFterminal}
実行ファイルの異なったパス名はノード名とそれぞれの実行ファイルへの
パス名を含む \verb|<p4pgFile>| ファイルを通してを指定できます．
例えばLinuxマシンarpとSolarisマシンnoddy上で
\OFtool{icoFoam}を実行するためには，
\verb|<p4pgFile>| は以下のように入力します．
\begin{OFfile}
\begin{verbatim}
arp  0 /usr/local/OpenFOAM/OpenFOAM-1.5/applications/bin/linuxOptMPICH/icoFoam
noddy 1 /usr/local/OpenFOAM/OpenFOAM-1.5/applications/bin/solarisOptMPICH/icoFoam
\end{verbatim}
\end{OFfile}
各行の2番目の入力，ここでは0と1，は各マシンあたりの追加プロセスの数です．
MPIの実行がarpから始められるので，マスタプロセスはそこで実行され，
他の追加プロセスはそこで始めてはいけません．
ジョブは実行によって始まります．
\begin{OFterminal}
\begin{verbatim}
mpirun  -p4pg <p4pgFile> `which <foamExec>`
<otherArgs> -parallel < /dev/null >& log &
\end{verbatim}
\end{OFterminal}
